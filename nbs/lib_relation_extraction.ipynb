{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp relation_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSpan:\n",
    "\n",
    "    def __init__(self, span):\n",
    "        self.span = span\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end_index - self.start_index\n",
    "\n",
    "    @property\n",
    "    def sentence(self):\n",
    "        return self.span.text\n",
    "\n",
    "    @property\n",
    "    def start_index(self):\n",
    "        return self.span.start\n",
    "    \n",
    "    @property\n",
    "    def end_index(self):\n",
    "        return self.span.end\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.sentence == other.sentence and self.start_index == self.start_index and self.end_index == self.end_index\n",
    "\n",
    "    def join(self, other):\n",
    "\n",
    "        if not self.intersects(other):\n",
    "            return None\n",
    "\n",
    "        min_start = min(self.start_index, other.start_index)\n",
    "        max_end = max(self.end_index, other.end_index)\n",
    "        \n",
    "        return TextSpan(self.span.doc[min_start:max_end])\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the subset of the start_index, end_index\n",
    "    start_index - the starting index of the subset, relative\n",
    "    to the spans parent document\n",
    "\n",
    "    end_index - the ending index of the subset, relative \n",
    "    to the spans partner document\n",
    "    \"\"\"    \n",
    "    def subset(self, start_index, end_index):\n",
    "        return TextSpan(self.span.doc[start_index:end_index])\n",
    "\n",
    "    \"\"\"\n",
    "    Returns true if the given span intersects with this span.\n",
    "    False otherwise.\n",
    "    \"\"\"\n",
    "    def intersects(self, other):\n",
    "        if self.start_index >= other.start_index and self.start_index <= other.end_index:\n",
    "            return True\n",
    "        if self.end_index >= other.start_index and self.end_index <= other.end_index:\n",
    "            return True\n",
    "        if other.start_index >= self.start_index and other.start_index <= self.end_index:\n",
    "            return True\n",
    "        if other.end_index >= self.start_index and other.end_index <= self.end_index:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "class Relation:\n",
    "\n",
    "    def __init__(self, left_phrase, relation_phrase, right_phrase):\n",
    "        \"\"\"Constructs a relation of the form\n",
    "        (left_phrase, relation_phrase, right_phrase)\n",
    "\n",
    "        Examples:\n",
    "        (Sean, runs to, mall), \n",
    "        (Gandalf, shall not, pass), \n",
    "        (the dog, flies, at midnight)\n",
    "\n",
    "        Args:\n",
    "            left_phrase (TextSpan): the leftside phrse\n",
    "            relation_phrase (TextSpan): the relation phrase\n",
    "            right_phrase (TextSpan): the right-side phrase of the relation\n",
    "        \"\"\"\n",
    "        self.left_phrase = left_phrase\n",
    "        self.relation_phrase = relation_phrase\n",
    "        self.right_phrase = right_phrase\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.left_phrase == other.left_phrase and self.relation_phrase == other.relation_phrase and self.right_phrase == other.right_phrase\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'({self.left_phrase.sentence}, {self.relation_phrase.sentence}, {self.right_phrase.sentence})'\n",
    "\n",
    "\n",
    "class RelationCollection:\n",
    "\n",
    "    def __init__(self, relations):\n",
    "        self.relations = relations\n",
    "\n",
    "    @property\n",
    "    def left_phrases(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def right_phrases(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def relation_phrases(self):\n",
    "        return None\n",
    "\n",
    "    def join(self, other):\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def construct_text_spans(doc, matches):\n",
    "    ret_spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        ret_spans.append(doc[start: end])\n",
    "    return ret_spans\n",
    "\n",
    "def extract_relations(doc):\n",
    "    \"\"\"extracts the complete relations from the doc\n",
    "\n",
    "    Args:\n",
    "        doc ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [Relation]: the complete set of relations found from the documentation\n",
    "    \"\"\"\n",
    "    relation_spans = get_relation_spans(doc)\n",
    "    noun_phrase_pattern = [[{\"POS\":\"NOUN\"}], [{\"POS\": \"PROPN\"}], [{\"POS\": \"PRON\"}]]\n",
    "    \n",
    "    relations = []\n",
    "\n",
    "    for span in relation_spans:\n",
    "        left_noun = find_nearest_pattern(doc, noun_phrase_pattern, span, True)\n",
    "        right_noun = find_nearest_pattern(doc, noun_phrase_pattern, span, False)\n",
    "\n",
    "        if (not left_noun is None) and (not right_noun is None):\n",
    "            relations.append(Relation(left_noun, span, right_noun))\n",
    "    return relations\n",
    "        \n",
    "\n",
    "\n",
    "def get_relation_spans(doc):\n",
    "    \"\"\"extracts the complete relations from the doc\n",
    "\n",
    "    Args:\n",
    "        doc (Document): the document we are using to gather\n",
    "        the middle portion of the relations\n",
    "\n",
    "    Returns:\n",
    "        [Relation]: the complete set of relations found from the documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    verbs = get_verbs(doc)\n",
    "    fluff_pattern = [[{\"POS\":\"VERB\"}, {\"POS\": \"PART\", \"OP\": \"*\"}, {\"POS\": \"ADV\", \"OP\":\"*\"}], \n",
    "                        [{\"POS\": \"VERB\"},  {\"POS\": \"ADP\", \"OP\": \"*\"}, {\"POS\": \"DET\", \"OP\":\"*\"},\n",
    "                        {\"POS\": \"AUX\", \"OP\": \"*\"},  \n",
    "                        {\"POS\": \"ADJ\", \"OP\":\"*\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}]]\n",
    "    #matcher = doc.matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"Fluff\", fluff_pattern)\n",
    "    syntactical_constraint_matches = construct_text_spans(doc, matcher(doc.doc))\n",
    "\n",
    "    relation_spans = []\n",
    "    for verb in verbs:\n",
    "        verb_spans = [span for span in syntactical_constraint_matches if verb in span.text]\n",
    "        joined_spans = merge_overlapping_consecutive_word_span(verb_spans)\n",
    "        longest_span = find_longest_span(joined_spans)\n",
    "        relation_spans.append(longest_span)\n",
    "    return relation_spans\n",
    "\n",
    "        \n",
    "\n",
    "def get_verbs(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    #matcher = doc.matcher\n",
    "    fluff_pattern = [[{\"POS\":\"VERB\"}]]\n",
    "    matcher.add(\"Fluff\", fluff_pattern)\n",
    "    matches = matcher(doc.doc)\n",
    "    verbs = []\n",
    "    for match_id, start, end in matches:\n",
    "        verbs.append(doc.doc[start:end].text)\n",
    "    return verbs\n",
    "\n",
    "def find_nearest_pattern(doc, pattern, text_span, search_before):\n",
    "    \"\"\"Find in doc, the nearest pattern to the given text_span,\n",
    "    returns the result as a TextSpan\n",
    "\n",
    "    Args:\n",
    "        doc (spacy Document) the document in spacy we are looking for\n",
    "        pattern (the pattern array to search for): the array of patterns we are\n",
    "        looking for\n",
    "        text_span (TextSpan): describes where in the document the word or phrase is\n",
    "        search_before (bool): if true, then we want to find the nearest pattern that occurs,\n",
    "                before text_span. Otherwise finds the nearest pattern after text_span\n",
    "    \"\"\"\n",
    "    #matcher = doc.matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"PatternNear\", pattern)\n",
    "    matches = matcher(doc.doc)\n",
    "    nearest_pattern = None\n",
    "    spans = construct_text_spans(doc, matches)\n",
    "    sorted_spans = sorted(spans, key=lambda s : s.start)\n",
    "\n",
    "    spans_to_search = []\n",
    "    if search_before:\n",
    "        spans_to_search = [span for span in sorted_spans if span.start < text_span.start]\n",
    "        spans_to_search.reverse()\n",
    "\n",
    "    else:\n",
    "        spans_to_search = [span for span in sorted_spans if span.start > text_span.start]\n",
    "\n",
    "    if len(spans_to_search) == 0:\n",
    "        return None\n",
    "\n",
    "    return spans_to_search[0]\n",
    "\n",
    "\n",
    "def merge_overlapping_consecutive_word_span(text_spans):\n",
    "    \"\"\"Merges two spans into one span iff they are\n",
    "    consecutive end_index=start_index or they overlap\n",
    "\n",
    "    Applies to all in order.\n",
    "\n",
    "    Args:\n",
    "        text_spans ([type]): the span containing the word\n",
    "    \"\"\"\n",
    "    sorted_spans = sorted(text_spans, key=lambda s : s.start)\n",
    "    current_index = 0\n",
    "    next_index = 1\n",
    "    merged_overlapping_spans = []\n",
    "    overlapped_indices = []\n",
    "\n",
    "    while next_index <= len(sorted_spans) - 1:\n",
    "\n",
    "        span = sorted_spans[current_index]\n",
    "        next_span = sorted_spans[next_index]\n",
    "        potential_overlap = None\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in filter_spans([span, next_span]):\n",
    "                potential_overlap = retokenizer.merge(span)\n",
    "        #potential_overlap = span.join(next_span)\n",
    "\n",
    "        if potential_overlap is None:\n",
    "            current_index = next_index\n",
    "            next_index = next_index + 1\n",
    "            merged_overlapping_spans.append(span)\n",
    "        else:\n",
    "            overlapped_indices.append(next_index)\n",
    "            sorted_spans[current_index] = potential_overlap\n",
    "            next_index = next_index + 1\n",
    "\n",
    "  \n",
    "    if next_index - current_index > 1:\n",
    "        merged_overlapping_spans.append(sorted_spans[current_index])\n",
    "\n",
    "    last_cons_index = len(sorted_spans) - 1\n",
    "    if not (last_cons_index in overlapped_indices):\n",
    "        merged_overlapping_spans.append(sorted_spans[last_cons_index])\n",
    "\n",
    "    return merged_overlapping_spans\n",
    "\n",
    "\n",
    "def find_latest_span(text_spans):\n",
    "    \"\"\"Finds the latest occuring span in given \n",
    "    set of text_spans\n",
    "\n",
    "    Args:\n",
    "        text_spans (TextSpan): the span of text according to some document\n",
    "    \"\"\"\n",
    "    if len(text_spans) == 0:\n",
    "        return None\n",
    "\n",
    "    sorted_spans = sorted(text_spans, key=lambda s: s.end_index, reverse=True)\n",
    "    return sorted_spans[0]\n",
    "\n",
    "def find_earliest_span(text_spans):\n",
    "    \"\"\"Finds the span that is the \"earliest occuriing\", i.e. the \n",
    "    smallest start index\n",
    "\n",
    "    Args:\n",
    "        text_spans ([type]): the smallest match on the text span\n",
    "    \"\"\"\n",
    "    if len(text_spans) == 0:\n",
    "        return None\n",
    "\n",
    "    sorted_spans = sorted(text_spans, key=lambda s: s.start_index)\n",
    "    return sorted_spans[0]\n",
    "\n",
    "\n",
    "def find_longest_span(text_spans):\n",
    "    \"\"\"find the longest match\n",
    "\n",
    "    Args:\n",
    "        text_spans ([TextSpan]): the set of matches we are filtering\n",
    "    \"\"\"\n",
    "    if len(text_spans) == 0:\n",
    "        return None\n",
    "    \n",
    "    return text_spans[0]\n",
    "    sorted_spans = sorted(text_spans, key=lambda s: len(s), reverse=True)\n",
    "    return sorted_spans[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text=\"\"\"\n",
    "In July 2012, Ancestry.com found a strong likelihood that Dunham was descended from John Punch\n",
    "\"\"\"\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E037] Error calculating span: Can't find a token ending at character offset 33.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2342432/4243624954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_relations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2342432/213462609.py\u001b[0m in \u001b[0;36mextract_relations\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRelation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrelations\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mrelation_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relation_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mnoun_phrase_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PROPN\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"POS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PRON\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2342432/213462609.py\u001b[0m in \u001b[0;36mget_relation_spans\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mrelation_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mverb_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntactical_constraint_matches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mjoined_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_overlapping_consecutive_word_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlongest_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_longest_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2342432/213462609.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mrelation_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mverb_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyntactical_constraint_matches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mjoined_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_overlapping_consecutive_word_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlongest_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_longest_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text_with_ws.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E037] Error calculating span: Can't find a token ending at character offset 33."
     ]
    }
   ],
   "source": [
    "relations = extract_relations(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_2342432/1624006205.py\u001b[0m(152)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    150 \u001b[0;31m    \u001b[0mspans_to_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msearch_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 152 \u001b[0;31m        \u001b[0mspans_to_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_spans\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtext_span\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    153 \u001b[0;31m        \u001b[0mspans_to_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    154 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
